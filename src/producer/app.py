import argparse
import json
import random
import time
import uuid
from dataclasses import asdict, dataclass
from datetime import datetime, timezone
from typing import Dict, Any, List, Optional

from faker import Faker

fake = Faker()

# Keep the domain values explicit and small on purpose:
# - Makes downstream schema validation predictable
# - Keeps queries/aggregations realistic without bloating cardinality
EVENT_TYPES = ["page_view", "add_to_cart", "purchase"]
PAGES = ["/", "/search", "/product", "/cart", "/checkout", "/help", "/pricing"]
BROWSERS = ["Chrome", "Firefox", "Safari", "Edge"]
OSES = ["iOS", "Android", "Windows", "macOS", "Linux"]
COUNTRIES = ["DE", "FR", "NL", "GB", "US", "PL", "SE", "ES", "IT"]


@dataclass
class ClickstreamEvent:
    """
    Canonical event representation for our synthetic clickstream.

    We keep the model close to how events usually look in production:
    - A stable event_id for idempotency/deduping
    - An event_ts that reflects client/server time (may be late/out-of-order)
    - User + session identifiers for funnels/sessionization
    - A flexible attributes bag for "long tail" fields (common in event pipelines)

    NOTE: event_ts is emitted as an ISO-8601 string in UTC to keep ingestion simple
    (and to avoid timezone bugs during early development).
    """
    event_id: str
    event_ts: str
    user_id: str
    session_id: str
    event_type: str
    page: str
    referrer: str
    device: Dict[str, str]
    geo: Dict[str, str]
    attributes: Dict[str, Any]


def iso_utc_now() -> str:
    """Current UTC timestamp formatted as ISO-8601 with millisecond precision."""
    return datetime.now(timezone.utc).isoformat(timespec="milliseconds")


def new_user_id() -> str:
    # In real systems user IDs are often opaque. We keep it readable for debugging.
    # Shortening the UUID reduces log noise while staying effectively unique for this demo.
    return f"usr_{uuid.uuid4().hex[:16]}"


def new_session_id() -> str:
    # Session IDs are typically generated by the client; here we emulate that behavior.
    return f"sess_{uuid.uuid4().hex[:16]}"


def make_event(user_id: str, session_id: str, event_type: Optional[str] = None) -> ClickstreamEvent:
    """
    Generate a single event.

    This function intentionally encodes a bit of "business realism":
    - Purchases should not come from /help, etc.
    - Commerce events include commerce attributes; page_view does not.

    Keeping the logic here (instead of in the session generator) makes it easy to
    reuse when we later add new event types or attribute sets.
    """
    etype = event_type or random.choice(EVENT_TYPES)

    # Page selection is biased by event type to keep the dataset plausible.
    page = random.choice(PAGES)
    if etype in ("add_to_cart", "purchase"):
        page = random.choice(["/product", "/cart", "/checkout"])

    # Referrer is simplified on purpose—enough for attribution examples later.
    referrer = random.choice(["direct", "google", "newsletter", "twitter", "linkedin", "partner_site"])

    device = {
        "os": random.choice(OSES),
        "browser": random.choice(BROWSERS),
    }

    # Geo is "semi-real": country from a fixed set, city from Faker (not country-consistent).
    # That's fine for analytics demos; if you need consistency, plug in a country->city map.
    country = random.choice(COUNTRIES)
    geo = {
        "country": country,
        "city": fake.city(),
    }

    # Attributes is where event schemas usually evolve over time.
    # We keep it small but representative so we can demonstrate schema evolution later.
    attrs: Dict[str, Any] = {
        "ab_test_variant": random.choice(["A", "B"]),
        "utm_campaign": random.choice(["brand", "summer_sale", "retargeting", "none"]),
    }

    # Commerce-ish payload for events that imply product intent.
    if etype in ("add_to_cart", "purchase"):
        attrs.update(
            {
                "product_id": f"sku_{random.randint(1000, 9999)}",
                "price": round(random.uniform(5.0, 300.0), 2),
                "currency": random.choice(["EUR", "USD", "GBP"]),
                "quantity": random.randint(1, 3),
            }
        )

    # Purchase adds a few extra fields so we can build funnels and revenue metrics in Gold.
    if etype == "purchase":
        attrs.update(
            {
                "order_id": f"ord_{uuid.uuid4().hex[:12]}",
                "payment_method": random.choice(["card", "paypal", "klarna"]),
            }
        )

    return ClickstreamEvent(
        event_id=str(uuid.uuid4()),
        event_ts=iso_utc_now(),
        user_id=user_id,
        session_id=session_id,
        event_type=etype,
        page=page,
        referrer=referrer,
        device=device,
        geo=geo,
        attributes=attrs,
    )


def generate_session_events(user_id: str, max_events: int) -> List[ClickstreamEvent]:
    """
    Generate a realistic-ish session sequence for a user.

    We purposely model a session as:
    - Starting with page_view
    - Followed by additional events with a heavy page_view bias
    - Low purchase probability (so conversions are sparse, like real traffic)

    Downstream, this lets us demonstrate:
    - session_id based aggregations
    - funnel logic (page_view -> add_to_cart -> purchase)
    - skewed distributions (lots of page views, few purchases)
    """
    session_id = new_session_id()
    events: List[ClickstreamEvent] = [make_event(user_id, session_id, "page_view")]

    n = random.randint(1, max_events)
    for _ in range(n - 1):
        # Bias towards page_view so the dataset doesn't become "too e-commerce heavy".
        etype = random.choices(
            population=["page_view", "add_to_cart", "purchase"],
            weights=[0.78, 0.18, 0.04],
            k=1,
        )[0]
        events.append(make_event(user_id, session_id, etype))

    return events


def main() -> None:
    # CLI arguments allow us to:
    # - quickly generate small datasets for unit tests
    # - simulate streaming vs batch by toggling --sleep-ms
    # - inject duplicates/out-of-order timestamps to validate Silver logic later
    parser = argparse.ArgumentParser(description="Generate clickstream events (JSONL).")
    parser.add_argument("--events", type=int, default=200, help="Total events to emit")
    parser.add_argument("--max-events-per-session", type=int, default=12, help="Max events per session")
    parser.add_argument("--sleep-ms", type=int, default=0, help="Sleep between events (simulate streaming)")
    parser.add_argument(
        "--out",
        type=str,
        default="-",
        help="Output path for JSONL. Use '-' for stdout (default).",
    )
    parser.add_argument(
        "--dup-rate",
        type=float,
        default=0.01,
        help="Probability [0..1] to emit a previously emitted event (tests dedupe in Silver).",
    )
    parser.add_argument(
        "--oo-rate",
        type=float,
        default=0.02,
        help="Probability [0..1] to emit an event with an older timestamp (tests late/out-of-order handling).",
    )
    args = parser.parse_args()

    # Keep startup logs separate from the event stream (especially when piping stdout).
    # This is useful later when stdout is fed directly into another process.
    print(f"[producer] starting with args={args}", flush=True)

    # Keep some state so we can intentionally emit duplicates.
    # We cap this list to avoid unbounded memory usage during long runs.
    previously_emitted: List[Dict[str, Any]] = []

    out_fh = None
    try:
        if args.out == "-":
            out_fh = None
        else:
            # Ensure the parent directory exists; the script should be runnable from repo root.
            import os
            parent_dir = os.path.dirname(args.out)
            if parent_dir:
                os.makedirs(parent_dir, exist_ok=True)
            out_fh = open(args.out, "w", encoding="utf-8")

        emitted = 0
        while emitted < args.events:
            # Generate a new user per session to keep it simple.
            # If you want more repeat visitors, replace this with a user pool.
            user_id = new_user_id()
            session_events = generate_session_events(user_id, args.max_events_per_session)

            for ev in session_events:
                if emitted >= args.events:
                    break

                payload = asdict(ev)

                # Inject out-of-order timestamps.
                # We shift event_ts backwards by 1–60 minutes to emulate clock skew / client buffering.
                if random.random() < args.oo_rate:
                    older = datetime.now(timezone.utc).timestamp() - random.randint(60, 3600)
                    payload["event_ts"] = datetime.fromtimestamp(older, tz=timezone.utc).isoformat(timespec="milliseconds")

                # Inject duplicates.
                # We reuse a full previously emitted payload, which reuses the same event_id.
                if previously_emitted and random.random() < args.dup_rate:
                    payload = random.choice(previously_emitted)

                # JSONL keeps ingestion and downstream parsing simple (one record per line).
                # Using compact separators reduces file size a bit.
                line = json.dumps(payload, separators=(",", ":"), ensure_ascii=False)

                if out_fh:
                    out_fh.write(line + "\n")
                else:
                    print(line)

                previously_emitted.append(payload)
                if len(previously_emitted) > 5000:
                    previously_emitted = previously_emitted[-2000:]

                emitted += 1

                # Throttle for a more "stream-like" flow. Useful when later pushing to Kinesis.
                if args.sleep_ms > 0:
                    time.sleep(args.sleep_ms / 1000.0)

    finally:
        # Always close the file handle if we opened one (even on Ctrl+C).
        if out_fh:
            out_fh.close()


if __name__ == "__main__":
    main()
